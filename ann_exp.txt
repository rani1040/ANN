														// odd/even  #exp1
def eo(x):
    alpha=1
    b,w1,w2,w3,w4=0,0,0,0,0
    x1=[-1,-1,-1,-1,-1,-1,-1,-1,1,1]
    x2=[-1,-1,-1,-1,1,1,1,1,-1,-1]
    x3=[-1,-1,1,1,-1,-1,1,1,-1,-1]
    x4=[-1,1,-1,1,-1,1,-1,1,-1,1]
    t=[-1,1,-1,1,-1,1,-1,1,-1,1]
    i=0
    yin=b+x1[0]*w1+x2[0]*w2+x3[0]*w3+x4[0]*w4
    if yin>=0:
        yin=1
    else:
        yin=-1
    while(yin!=t[i]):
        print("value of yin and t is", yin, t[i])
        b+=alpha*t[i];
        w1+=alpha*x1[i]*t[i]
        w2+=alpha*x2[i]*t[i]
        w3+=alpha*x3[i]*t[i]
        w4+=alpha*x4[i]*t[i]
        print("values of b w1 w2 w3 w4", b,w1,w2,w3,w4)
        i+=1
        yin=b+x1[0]*w1+x2[0]*w2+x3[0]*w3+x4[0]*w4
        if yin>=0:
            yin=1
        else:
            yin=-1

        y=x%10
        k=0
        if y==0:
            bina=[0,0,0,0]
        else:
            bina=[0,0,0,0]
            while y!=0:
                bina[k]=y%2
                y=int(y/2)
                k+=1
    net=b+bina[3]*w1+bina[2]*w2+bina[1]*w3+bina[0]*w4
    print("net value",net)
    if net >0:
        print("Number is odd")
    else:
        print("Number is even")
    
eo(int(input("Enter number : ")))






                                                                                 //final and gate  #exp2(a)
def AND_Gate(w1,w2,bias):
    alpha=1
    yin=[]
    x1=[1,1,-1,-1]
    x2=[1,-1,1,-1]
    t=[1,-1,-1,-1]
    ite=1
    while (t!=yin):
        
        yin=[]
        for i in range(4):
            y=bias+x1[i]*w1+x2[i]*w2
            if y>=1:
                o=1
            else:
                o=-1
            yin.append(o)
            w1=w1+alpha*(t[i]-o)*x1[i]
            w2=w2+alpha*(t[i]-o)*x2[i]
            bias=bias+alpha*(t[i]-o)
        print("bias,w1,w2",bias,w1,w2)
        ite=ite+1
        print("yin",yin)
    

    
w1=float(input("Enter w1 : "))
w2=float(input("Enter w2 : "))
bias=float(input("Enter bias : "))
AND_Gate(w1,w2,bias)




														// OR gate  #exp2(b)
def or_gate(w1,w2,bias):
    alpha=1
    yin=[]
    
    x1=[1,1,-1,-1]
    x2=[1,-1,1,-1]
    t=[1,1,1,-1]
    ite=1
    while (t!=yin):
        #print("yin",yin)
        yin=[]
        for i in range(4):
            y=bias+x1[i]*w1+x2[i]*w2
            if y>=1:
                o=1
            else:
                o=-1
            yin.append(o)
            w1=w1+alpha*(t[i]-o)*x1[i]
            w2=w2+alpha*(t[i]-o)*x2[i]
            bias=bias+alpha*(t[i]-o)
        print("bias,w1,w2",bias,w1,w2)
        ite=ite+1
        print("yin",yin)

    
w1=float(input("Enter w1 : "))
w2=float(input("Enter w2 : "))
bias=float(input("Enter bias : "))
or_gate(w1,w2,bias)



													// PERCEPTRON training algorithm exp(3)

import pandas as pd
from sklearn.linear_model import Perceptron
diabetes=pd.read_csv('../csv/diabetes.csv')
x=diabetes.drop(columns='Outcome').values
y=diabetes['Outcome'].values
model=Perceptron(random_state=1)
model.fit(x,y)
model.score(x,y)

														    // DELTA rule exp (5)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

class PERCEPTRON:
    def __init__(self):
        self.weights=[]
    
    def activation(self,data):
        activation_val=np.dot(self.weights,data)
        return 1 if activation_val>=0 else 0
    
    def fit(self,x,y,lrate,epochs):
        #initialize weight vector
        self.weights=[0.0 for i in range(len(x.columns))]
        #no. of iterations to train the neural network
        for epoch in range(epochs):
            print(str(epoch+1),'epoch has started')
            for index in range(len(x)):
                temp=x.iloc[index]
                predicted=self.activation(temp)
                if y.iloc[index]==predicted:
                    pass
                else:
                    # calculate the error value
                    error=y.iloc[index]-predicted
                        
                    #updatation of associated self.weights according to delta rule
                    for j in range(len(temp)):
                        self.weights[j]=self.weights[j]+lrate*error*temp[j]
                        
    def predict(self,x_test):
        predicted=[]
        for i in range(len(x_test)):
            #prediction for test-set using obtained weights
            predicted.append(self.activation(x_test.iloc[i]))
        return predicted
    
    def accuracy(self,predicted,original):
        correct=0
        lent=len(predicted)
        for i in range(lent):
            if predicted[i]==original.iloc[i]:
                correct+=1
        return (correct/lent)*100
    
    def get_weights(self):
        return self.weights
    
obj=PERCEPTRON()
df=pd.read_csv('../csv/Iris.csv')
df.columns=["petal_length","petal_width","sepal_length","sepal_width","class"]
y=df['class']
x=df.drop(columns='class')

train_x,test_x,train_y,test_y=train_test_split(x,y)
obj.fit(train_x,train_y,0.5,10)
pred_y=obj.predict(test_x)
print('accuracy',obj.accuracy(pred_y,test_y))
print('weights',obj.get_weights())






															// HEBB rule exp(6)

import numpy as np
e=[1,1,1,1,1,-1,-1,-1,1,1,-1,1,1,-1,-1,-1,1,1,1,1]
f=[1,1,1,1,1,-1,-1,-1,1,1,1,1,1,-1,-1,-1,1,-1,-1,-1]
w=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
t=[1,-1]
b=0
alpha=0.1
we=[]
for i in range(0,20):
    d=w[i]+e[i]*t[0]+f[i]*t[1]
    b=b+t[0]+t[1]
    we.append(d)

E=np.array(e)
W=np.array(we)
ff=np.array(f)
ypre=np.dot(W,E.T)
if ypre>=0:
    pred=1
else:
    pred=-1
print(pred)
ypre=np.dot(W,ff.T)
if ypre>0:
    pred=1
else:
    pred=-1
print(pred)




														// BAM exp(7)

import numpy as np
 
# Take two sets of patterns:
# Set A: Input Pattern
x1 = np.array([1, 1, 1, 1, 1, 1]).reshape(6, 1)
x2 = np.array([-1, -1, -1, -1, -1, -1]).reshape(6, 1)
x3 = np.array([1, 1, -1, -1, 1, 1]).reshape(6, 1)
x4 = np.array([-1, -1, 1, 1, -1, -1]).reshape(6, 1)
 
# Set B: Target Pattern
y1 = np.array([1, 1, 1]).reshape(3, 1)
y2 = np.array([-1, -1, -1]).reshape(3, 1)
y3 = np.array([1, -1, 1]).reshape(3, 1)
y4 = np.array([-1, 1, -1]).reshape(3, 1)
 
# Calculate weight Matrix: W
inputSet = np.concatenate((x1, x2, x3, x4), axis = 1)
targetSet = np.concatenate((y1.T, y2.T, y3.T, y4.T), axis = 0)
print("\nWeight matrix:")
weight = np.dot(inputSet, targetSet)
print(weight)

print("\nTesting for input patterns: Set A")
def testInputs(x, weight):
  # Multiply the input pattern with the weight matrix
  # (weight.T X x)
  y = np.dot(weight.T, x)
  y[y < 0] = -1
  y[y >= 0] = 1
  return np.array(y)
 
print("\nOutput of input pattern 1")
print(testInputs(x1, weight))
print("\nOutput of input pattern 2")
print(testInputs(x2, weight))
print("\nOutput of input pattern 3")
print(testInputs(x3, weight))
print("\nOutput of input pattern 4")
print(testInputs(x4, weight))
 
# Test for Target Patterns: Set B
print("\nTesting for target patterns: Set B")
def testTargets(y, weight):
    x = np.dot(weight, y)
    x[x <= 0] = -1
    x[x > 0] = 1
    return np.array(x)
print("\nOutput of target pattern 1")
print(testTargets(y1, weight))
print("\nOutput of target pattern 2")
print(testTargets(y2, weight))
print("\nOutput of target pattern 3")
print(testTargets(y3, weight))
print("\nOutput of target pattern 4")
print(testTargets(y4, weight))



													// BACKPROPOGATION exp(8)

import numpy as np
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)
X = X/np.amax(X,axis=0) # maximum of X array longitudinally
y = y/100

#sigmoid function
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))

#derivative of sigmoid function
def derivatives_sigmoid(x):
    return x*(1-x)

# varaiable initialization
epoch=5
lr=0.1
inputlayer_neurons=2
hiddenlayer_neurons=3
output_neurons=1

#weight and bias initialization
wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
bh=np.random.uniform(size=(1,hiddenlayer_neurons))
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))

#draws a random range of numbers uniformly of dim x*y
for i in range(epoch):
    
#Forward Propogation
    hinp1=np.dot(X,wh)
    hinp=hinp1 + bh
    hlayer_act = sigmoid(hinp)
    outinp1=np.dot(hlayer_act,wout)
    outinp= outinp1+ bout
    output = sigmoid(outinp)
    
#Backpropagation
    EO = y-output
    outgrad = derivatives_sigmoid(output)
    d_output = EO* outgrad
    EH = d_output.dot(wout.T)

#how much hidden layer wts contributed to error
    hiddengrad = derivatives_sigmoid(hlayer_act)
    d_hiddenlayer = EH * hiddengrad
    
# dotproduct of nextlayererror and currentlayerop
    wout += hlayer_act.T.dot(d_output) *lr
    wh += X.T.dot(d_hiddenlayer) *lr

print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n" ,output)